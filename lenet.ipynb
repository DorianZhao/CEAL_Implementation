{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Callable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import softmax\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.optim as Optimizer\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple lenet model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(256, 120)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.relu5 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)\n",
    "        y = self.relu1(y)\n",
    "        y = self.pool1(y)\n",
    "        y = self.conv2(y)\n",
    "        y = self.relu2(y)\n",
    "        y = self.pool2(y)\n",
    "        y = y.view(y.shape[0], -1)\n",
    "        y = self.fc1(y)\n",
    "        y = self.relu3(y)\n",
    "        y = self.fc2(y)\n",
    "        y = self.relu4(y)\n",
    "        y = self.fc3(y)\n",
    "        y = self.relu5(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encapsulate LeNet\n",
    "class LeNet(object):\n",
    "    def __init__(self, n_classes = 10, device = None):\n",
    "        self.n_classes = n_classes\n",
    "        if device is None:\n",
    "            self.device = torch.device(\n",
    "                \"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = Model()\n",
    "        \n",
    "    def __train_one_epoch__(self, train_loader, optimizer, criterion,\n",
    "                        valid_loader = None, epoch = 0, each_batch_idx = 0):\n",
    "        train_loss = 0\n",
    "        data_size = 0\n",
    "        \n",
    "        for batch_idx, (img, label) in enumerate(train_loader):\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # run forward\n",
    "            pred_prob = self.model(img)\n",
    "\n",
    "            # calculate loss\n",
    "            loss = criterion(pred_prob, label)\n",
    "\n",
    "            # calculate gradient (backprop)\n",
    "            loss.backward()\n",
    "\n",
    "            # total train loss\n",
    "            train_loss += loss.item()\n",
    "            data_size += label.size(0)\n",
    "\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "        if valid_loader:\n",
    "            acc = self.evaluate(test_loader=valid_loader)\n",
    "            print('Accuracy on the valid dataset {}'.format(acc))\n",
    "            \n",
    "    def train(self, epochs, train_loader, valid_loader = None):\n",
    "        self.model.train()\n",
    "        optimizer = optim.SGD(\n",
    "            filter(lambda p: p.requires_grad, self.model.parameters()),\n",
    "            lr=0.001, momentum=0.9)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        for epoch in range(epochs):\n",
    "            self.__train_one_epoch__(train_loader=train_loader,\n",
    "                                   optimizer=optimizer,\n",
    "                                   criterion=criterion,\n",
    "                                   valid_loader=valid_loader,\n",
    "                                   epoch=epoch\n",
    "                                   )\n",
    "\n",
    "    def evaluate(self, test_loader):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data,labels) in enumerate(test_loader):\n",
    "                data = data.float()\n",
    "                outputs = self.model(data)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        return 100 * correct / total\n",
    "\n",
    "    def predict(self, test_loader):\n",
    "        self.model.eval()\n",
    "        predict_results = np.empty(shape=(0, 10))\n",
    "        with torch.no_grad():\n",
    "            for batch_idx,  (img, label) in enumerate(test_loader):\n",
    "                outputs = self.model(img)\n",
    "                outputs = softmax(outputs)\n",
    "                predict_results = np.concatenate(\n",
    "                    (predict_results, outputs.cpu().numpy()))\n",
    "        return predict_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain mnist dataset\n",
    "mnist_train = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), target_transform=None, download=False)\n",
    "mnist_test = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), target_transform=None, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataloader of MNIST\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "random_seed = 123\n",
    "validation_split = 0.1  \n",
    "batch_size = 16\n",
    "dataset_size = len(mnist_train)\n",
    "\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "dl_train = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, sampler=train_sampler)\n",
    "dl_test = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, sampler=valid_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = LeNet(n_classes = 10)\n",
    "test_model.train(epochs = 1, train_loader = dl_train, valid_loader = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.71666666666667\n"
     ]
    }
   ],
   "source": [
    "acc = test_model.evaluate(test_loader = dl_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
